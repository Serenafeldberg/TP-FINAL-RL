"""
Script para visualizar las m√©tricas de entrenamiento de PPO.

Grafica:
- Recompensas por episodio (con promedio m√≥vil)
- Longitud de episodios
- P√©rdidas (policy, value, entropy)
- M√©tricas adicionales (clip fraction, KL divergence, learning rate)
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path

# Configuraci√≥n de rutas
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
REWARDS_CSV = PROJECT_ROOT / "logs" / "rewards.csv"
LOSSES_CSV = PROJECT_ROOT / "logs" / "losses.csv"


def moving_average(data, window_size=100):
    """Calcular promedio m√≥vil."""
    if len(data) < window_size:
        return data
    return pd.Series(data).rolling(window=window_size, min_periods=1).mean().values


def plot_rewards(rewards_df, window_size=100, save_path=None):
    """
    Graficar recompensas por episodio.
    
    Args:
        rewards_df: DataFrame con columnas timestep, episode, reward, length
        window_size: tama√±o de ventana para promedio m√≥vil
        save_path: ruta para guardar la figura (opcional)
    """
    fig, axes = plt.subplots(2, 1, figsize=(12, 8))
    
    # Gr√°fico 1: Recompensas
    ax1 = axes[0]
    episodes = rewards_df['episode'].values
    rewards = rewards_df['reward'].values
    
    # Recompensas individuales (transparentes)
    ax1.plot(episodes, rewards, alpha=0.3, color='blue', label='Recompensa por episodio')
    
    # Promedio m√≥vil
    if len(rewards) >= window_size:
        ma_rewards = moving_average(rewards, window_size)
        ax1.plot(episodes, ma_rewards, color='red', linewidth=2, 
                label=f'Promedio m√≥vil ({window_size} episodios)')
    
    ax1.set_xlabel('Episodio', fontsize=12)
    ax1.set_ylabel('Recompensa', fontsize=12)
    ax1.set_title('Recompensas por Episodio', fontsize=14, fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Estad√≠sticas en el gr√°fico
    if len(rewards) > 0:
        final_avg = np.mean(rewards[-window_size:]) if len(rewards) >= window_size else np.mean(rewards)
        max_reward = np.max(rewards)
        min_reward = np.min(rewards)
        ax1.text(0.02, 0.98, 
                f'Promedio final ({window_size} √∫ltimos): {final_avg:.2f}\n'
                f'M√°xima: {max_reward:.2f}\n'
                f'M√≠nima: {min_reward:.2f}',
                transform=ax1.transAxes, fontsize=10,
                verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    # Gr√°fico 2: Longitud de episodios
    ax2 = axes[1]
    lengths = rewards_df['length'].values
    
    ax2.plot(episodes, lengths, alpha=0.3, color='green', label='Longitud por episodio')
    
    if len(lengths) >= window_size:
        ma_lengths = moving_average(lengths, window_size)
        ax2.plot(episodes, ma_lengths, color='orange', linewidth=2,
                label=f'Promedio m√≥vil ({window_size} episodios)')
    
    ax2.set_xlabel('Episodio', fontsize=12)
    ax2.set_ylabel('Longitud del Episodio', fontsize=12)
    ax2.set_title('Longitud de Episodios', fontsize=14, fontweight='bold')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Gr√°fico guardado en: {save_path}")
    
    return fig


def plot_losses(losses_df, save_path=None):
    """
    Graficar p√©rdidas y m√©tricas de entrenamiento.
    
    Args:
        losses_df: DataFrame con columnas timestep, policy_loss, value_loss, etc.
        save_path: ruta para guardar la figura (opcional)
    """
    fig, axes = plt.subplots(3, 2, figsize=(14, 10))
    
    timesteps = losses_df['timestep'].values
    
    # 1. Policy Loss
    ax = axes[0, 0]
    ax.plot(timesteps, losses_df['policy_loss'], color='blue', linewidth=1.5)
    ax.set_xlabel('Timestep', fontsize=10)
    ax.set_ylabel('Policy Loss', fontsize=10)
    ax.set_title('Policy Loss', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # 2. Value Loss
    ax = axes[0, 1]
    ax.plot(timesteps, losses_df['value_loss'], color='red', linewidth=1.5)
    ax.set_xlabel('Timestep', fontsize=10)
    ax.set_ylabel('Value Loss', fontsize=10)
    ax.set_title('Value Loss', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3)
    # Escala logar√≠tmica si los valores son muy grandes
    if losses_df['value_loss'].max() > 100:
        ax.set_yscale('log')
    
    # 3. Entropy Loss (negativo de la entrop√≠a)
    ax = axes[1, 0]
    entropy = -losses_df['entropy_loss']  # Convertir a entrop√≠a positiva
    ax.plot(timesteps, entropy, color='green', linewidth=1.5)
    ax.set_xlabel('Timestep', fontsize=10)
    ax.set_ylabel('Entropy', fontsize=10)
    ax.set_title('Entropy (Exploraci√≥n)', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # 4. Clip Fraction
    ax = axes[1, 1]
    ax.plot(timesteps, losses_df['clip_fraction'], color='orange', linewidth=1.5)
    ax.set_xlabel('Timestep', fontsize=10)
    ax.set_ylabel('Clip Fraction', fontsize=10)
    ax.set_title('Clip Fraction (Ratio de clipping)', fontsize=12, fontweight='bold')
    ax.set_ylim([0, 1])
    ax.grid(True, alpha=0.3)
    
    # 5. Approximate KL Divergence
    ax = axes[2, 0]
    ax.plot(timesteps, losses_df['approx_kl'], color='purple', linewidth=1.5)
    ax.set_xlabel('Timestep', fontsize=10)
    ax.set_ylabel('Approx KL', fontsize=10)
    ax.set_title('Approximate KL Divergence', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3)
    # L√≠nea de referencia para KL alto (indica actualizaciones muy grandes)
    ax.axhline(y=0.01, color='r', linestyle='--', alpha=0.5, label='Umbral (0.01)')
    ax.legend(fontsize=8)
    
    # 6. Learning Rate
    ax = axes[2, 1]
    ax.plot(timesteps, losses_df['learning_rate'], color='brown', linewidth=1.5)
    ax.set_xlabel('Timestep', fontsize=10)
    ax.set_ylabel('Learning Rate', fontsize=10)
    ax.set_title('Learning Rate (con decay)', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Gr√°fico guardado en: {save_path}")
    
    return fig


def plot_combined_summary(rewards_df, losses_df, window_size=100, save_path=None):
    """
    Crear un resumen combinado con las m√©tricas m√°s importantes.
    
    Args:
        rewards_df: DataFrame de recompensas
        losses_df: DataFrame de p√©rdidas
        window_size: tama√±o de ventana para promedio m√≥vil
        save_path: ruta para guardar la figura (opcional)
    """
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. Recompensas (arriba izquierda)
    ax = axes[0, 0]
    episodes = rewards_df['episode'].values
    rewards = rewards_df['reward'].values
    ax.plot(episodes, rewards, alpha=0.3, color='blue')
    if len(rewards) >= window_size:
        ma_rewards = moving_average(rewards, window_size)
        ax.plot(episodes, ma_rewards, color='red', linewidth=2,
                label=f'Promedio m√≥vil ({window_size})')
    ax.set_xlabel('Episodio')
    ax.set_ylabel('Recompensa')
    ax.set_title('Recompensas', fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 2. Policy Loss (arriba derecha)
    ax = axes[0, 1]
    timesteps = losses_df['timestep'].values
    ax.plot(timesteps, losses_df['policy_loss'], color='blue', linewidth=1.5)
    ax.set_xlabel('Timestep')
    ax.set_ylabel('Policy Loss')
    ax.set_title('Policy Loss', fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # 3. Value Loss (abajo izquierda)
    ax = axes[1, 0]
    ax.plot(timesteps, losses_df['value_loss'], color='red', linewidth=1.5)
    ax.set_xlabel('Timestep')
    ax.set_ylabel('Value Loss')
    ax.set_title('Value Loss', fontweight='bold')
    ax.grid(True, alpha=0.3)
    if losses_df['value_loss'].max() > 100:
        ax.set_yscale('log')
    
    # 4. Entropy (abajo derecha)
    ax = axes[1, 1]
    entropy = -losses_df['entropy_loss']
    ax.plot(timesteps, entropy, color='green', linewidth=1.5)
    ax.set_xlabel('Timestep')
    ax.set_ylabel('Entropy')
    ax.set_title('Entropy (Exploraci√≥n)', fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Resumen guardado en: {save_path}")
    
    return fig


def main():
    """Funci√≥n principal."""
    print("=" * 60)
    print("VISUALIZACI√ìN DE M√âTRICAS DE ENTRENAMIENTO PPO")
    print("=" * 60)
    
    # Verificar que los archivos existan
    if not REWARDS_CSV.exists():
        print(f"‚ùå Error: No se encontr√≥ {REWARDS_CSV}")
        return
    
    if not LOSSES_CSV.exists():
        print(f"‚ùå Error: No se encontr√≥ {LOSSES_CSV}")
        return
    
    # Leer datos
    print(f"\nüìä Leyendo datos...")
    rewards_df = pd.read_csv(REWARDS_CSV)
    losses_df = pd.read_csv(LOSSES_CSV)
    
    print(f"  ‚úì Recompensas: {len(rewards_df)} episodios")
    print(f"  ‚úì P√©rdidas: {len(losses_df)} updates")
    
    # Crear directorio de salida si no existe
    output_dir = SCRIPT_DIR / "output"
    output_dir.mkdir(exist_ok=True)
    
    # Graficar recompensas
    print(f"\nüìà Generando gr√°fico de recompensas...")
    fig_rewards = plot_rewards(rewards_df, window_size=100, 
                              save_path=output_dir / "rewards.png")
    
    # Graficar p√©rdidas
    print(f"\nüìâ Generando gr√°fico de p√©rdidas...")
    fig_losses = plot_losses(losses_df, 
                            save_path=output_dir / "losses.png")
    
    # Resumen combinado
    print(f"\nüìä Generando resumen combinado...")
    fig_summary = plot_combined_summary(rewards_df, losses_df, window_size=100,
                                        save_path=output_dir / "summary.png")
    
    print(f"\n‚úÖ Visualizaciones completadas!")
    print(f"   Archivos guardados en: {output_dir}")
    print(f"\n   - rewards.png: Recompensas y longitudes")
    print(f"   - losses.png: Todas las p√©rdidas y m√©tricas")
    print(f"   - summary.png: Resumen combinado")
    
    # Mostrar estad√≠sticas
    print(f"\nüìä Estad√≠sticas:")
    print(f"   Recompensas:")
    print(f"     - Promedio: {rewards_df['reward'].mean():.2f}")
    print(f"     - M√°xima: {rewards_df['reward'].max():.2f}")
    print(f"     - M√≠nima: {rewards_df['reward'].min():.2f}")
    print(f"     - √öltimos 100 episodios: {rewards_df['reward'].tail(100).mean():.2f}")
    
    print(f"\n   P√©rdidas:")
    print(f"     - Policy Loss (promedio): {losses_df['policy_loss'].mean():.4f}")
    print(f"     - Value Loss (promedio): {losses_df['value_loss'].mean():.4f}")
    print(f"     - Entropy (promedio): {-losses_df['entropy_loss'].mean():.4f}")
    print(f"     - Clip Fraction (promedio): {losses_df['clip_fraction'].mean():.4f}")
    
    # Mostrar gr√°ficos
    plt.show()


if __name__ == "__main__":
    main()

